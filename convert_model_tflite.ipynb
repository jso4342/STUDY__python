{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO3aR+KxhcaqFISZOHtJSPT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jso4342/STUDY__python/blob/main/convert_model_tflite.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "VnPp-1egbhAc"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.mobile_optimizer import optimize_for_mobile"
      ],
      "metadata": {
        "id": "YvzEibAsVvCP"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision\n",
        "import torchvision.models\n",
        "import torchvision.models.segmentation"
      ],
      "metadata": {
        "id": "pWkkEkQvg9nL"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pth_model_path = '/content/model_mbv3_iou_mix_2C049.pth'\n",
        "device = torch.device('cpu')"
      ],
      "metadata": {
        "id": "mAegEw0_bjPq"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = torch.load(pth_model_path, map_location=device)"
      ],
      "metadata": {
        "id": "bFIj9j6Ab8ta"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        },
        "id": "yqBxfPdy5c8J",
        "outputId": "5bd4d67b-6349-4b23-e560-646dd522228b"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-40-67f96fad7f04>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'collections.OrderedDict' object has no attribute 'eval'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "scriptedm = torch.jit.script(model)"
      ],
      "metadata": {
        "id": "4JSAyokvbefj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scriptedm.save(\"deeplabv3_scripted.pt\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 172
        },
        "id": "j-zFIBIKb9A6",
        "outputId": "7c903b04-b6ee-4698-c988-fd10ce13063e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-4f4b96b2b3b2>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mscriptedm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"deeplabv3_scripted.pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'torch._C.ScriptDict' object has no attribute 'save'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.jit.save(scriptedm, \"torch_jit.pt\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        },
        "id": "WEUZegwpbiv8",
        "outputId": "6776d613-9950-40a4-be88-9d90d3b0c2e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-f0ecd7e7e122>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscriptedm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"torch_jit.pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/jit/_serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(m, f, _extra_files)\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0m_extra_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpathlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m         \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_extra_files\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_extra_files\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_to_buffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_extra_files\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_extra_files\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'torch._C.ScriptDict' object has no attribute 'save'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class TempModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        self.conv1 = nn.Conv2d(3, 5, (3, 3))\n",
        "    def forward(self, inp):\n",
        "        return self.conv1(inp)\n",
        "\n",
        "model = TempModel()\n",
        "model.load_state_dict(torch.load(file_path))\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "egQa8gHpWaEK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 172
        },
        "id": "jbY64uGZVPRP",
        "outputId": "c8b8486e-bf6f-4e92-fe37-3b81125f8aef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-cc5fc219c64f>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mthe_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'collections.OrderedDict' object has no attribute 'eval'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "scripted_module = torch.jit.script(model)"
      ],
      "metadata": {
        "id": "8Qah_I8Ac6t4",
        "outputId": "15e39811-f58a-453f-a260-ca334702c21f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/jit/_recursive.py:260: UserWarning: 'aux_classifier' was found in ScriptModule constants,  but it is a non-constant submodule. Consider removing it.\n",
            "  warnings.warn(\"'{}' was found in ScriptModule constants, \"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "scripted_module.save(\"deeplabv3_scripted.pt\")"
      ],
      "metadata": {
        "id": "ldPEx_LkP-Ht"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimized_scripted_module = optimize_for_mobile(scripted_module)"
      ],
      "metadata": {
        "id": "pBkvSuzGQC7q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimized_scripted_module._save_for_lite_interpreter(\"deeplabv3_scripted.ptl\")"
      ],
      "metadata": {
        "id": "whpsVTOdVdHH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Export mobile interpreter version model (compatible with mobile interpreter)\n",
        "optimized_scripted_module = optimize_for_mobile(scripted_module)\n",
        "optimized_scripted_module._save_for_lite_interpreter(\"deeplabv3_scripted.ptl\")"
      ],
      "metadata": {
        "id": "ONiEkMENc9PM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.mobile_optimizer import optimize_for_mobile\n",
        "\n",
        "# 편의를 위해 torch.hub에서 모델을 가져왔지만, pretrained 한 모델 또한 사용 가능\n",
        "model = torch.hub.load('pytorch/vision:v0.7.0', 'deeplabv3_resnet50', pretrained=True)\n",
        "model.eval()\n",
        "\n",
        "scriptedm = torch.jit.script(model)\n",
        "scriptedm.save(\"deeplabv3_scripted.pt\") \t# 기존 pt 모델 저장\n",
        "optimized_scripted_module = optimize_for_mobile(scriptedm)\n",
        "optimized_scripted_module._save_for_lite_interpreter(\"deeplabv3_scripted_lite.ptl\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F79aC8rFQKGC",
        "outputId": "bba639f0-90a9-4b21-c45e-65a4da9684b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.7.0\n",
            "/usr/local/lib/python3.10/dist-packages/torch/jit/_recursive.py:260: UserWarning: 'aux_classifier' was found in ScriptModule constants,  but it is a non-constant submodule. Consider removing it.\n",
            "  warnings.warn(\"'{}' was found in ScriptModule constants, \"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install models"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9QPVK30yeSIE",
        "outputId": "f445a411-2c1a-4866-89bc-e7468d12303c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting models\n",
            "  Downloading models-0.9.3.tar.gz (16 kB)\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from models.generator import Generator\n",
        "from torch.utils.mobile_optimizer import optimize_for_mobile\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 372
        },
        "id": "6EiQOO8beQW9",
        "outputId": "c9872ab1-635b-4abd-dc24-393f52cf4166"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-f9daa18136a6>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerator\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGenerator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmobile_optimizer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0moptimize_for_mobile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'models'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from models.generator import Generator\n",
        "from torch.utils.mobile_optimizer import optimize_for_mobile\n",
        "\n",
        "device = torch.device('cpu')\n",
        "\n",
        "\n",
        "\n",
        "ModelName = \"trained_netG.pth\"\n",
        "example_input = torch.rand(1, 3, 452, 340, dtype=torch.float)\n",
        "model2  = Generator().to(device)\n",
        "model2.load_state_dict(torch.load(ModelName, map_location=device))\n",
        "model2.eval()\n",
        "script_module = torch.jit.trace(model2.forward, example_input)\n",
        "\n",
        "\n",
        "torchscript_model_optimized = optimize_for_mobile(script_module)\n",
        "torch.jit.save(torchscript_model_optimized, \"cartoon_test2.pt\")\n",
        "\n",
        "script_module.save('cartoon_test.pt')\n",
        "\n",
        "\n",
        "loaded_model = torch.jit.load(\"cartoon_test.pt\")\n",
        "loaded_model.eval()\n",
        "loaded_model.save('cartoon_test3.pt')"
      ],
      "metadata": {
        "id": "mqw0MqHZeOQ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install onnx_tf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BKvouLSUhNos",
        "outputId": "d07ff8c5-2d08-4566-a6dc-8f184d649511"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting onnx_tf\n",
            "  Downloading onnx_tf-1.10.0-py3-none-any.whl (226 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m226.1/226.1 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting onnx>=1.10.2 (from onnx_tf)\n",
            "  Downloading onnx-1.14.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.6/14.6 MB\u001b[0m \u001b[31m81.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from onnx_tf) (6.0)\n",
            "Collecting tensorflow-addons (from onnx_tf)\n",
            "  Downloading tensorflow_addons-0.20.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (591 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m591.0/591.0 kB\u001b[0m \u001b[31m37.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from onnx>=1.10.2->onnx_tf) (1.22.4)\n",
            "Requirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.10/dist-packages (from onnx>=1.10.2->onnx_tf) (3.20.3)\n",
            "Requirement already satisfied: typing-extensions>=3.6.2.1 in /usr/local/lib/python3.10/dist-packages (from onnx>=1.10.2->onnx_tf) (4.5.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow-addons->onnx_tf) (23.1)\n",
            "Collecting typeguard<3.0.0,>=2.7 (from tensorflow-addons->onnx_tf)\n",
            "  Downloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n",
            "Installing collected packages: typeguard, onnx, tensorflow-addons, onnx_tf\n",
            "Successfully installed onnx-1.14.0 onnx_tf-1.10.0 tensorflow-addons-0.20.0 typeguard-2.13.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convert ONNX model to TensorFlow format"
      ],
      "metadata": {
        "id": "LtKsXY3gj6Tl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from onnx_tf.backend import prepare\n",
        "import torch.onnx\n",
        "import onnx\n",
        "import onnx_tf"
      ],
      "metadata": {
        "id": "eAlcLjcAhK7n"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "onnx_model_path = \"/content/deeplabv3_onnx_model.onnx\"\n",
        "onnx_model = onnx.load(onnx_model_path)\n",
        "tf_rep = onnx_tf.backend.prepare(onnx_model)\n",
        "tf_model_path = \"/content/output/deeplabv3_tensorflow_from_onnx.pb\"\n",
        "tf_rep.export_graph(tf_model_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "keCjkMucg6kC",
        "outputId": "ae536592-8c4f-4c1d-f9cc-f0534ee5ab9e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as gen_tensor_dict while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convert Tensorflow model to Tensorflow Lite format"
      ],
      "metadata": {
        "id": "_-3ugP_9mT0Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "A6I3kq8qmecI"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tflite_model_path = \"deeplabv3_tensorflow_lite.tflite\"\n",
        "converter = tf.lite.TFLiteConverter.from_saved_model(tf_model_path)\n",
        "tflite_model = converter.convert()\n",
        "with open(tflite_model_path, \"wb\") as f:\n",
        "    f.write(tflite_model)"
      ],
      "metadata": {
        "id": "gzSt1WGymY-Z"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "양자화"
      ],
      "metadata": {
        "id": "iWxfG2IBn0Ts"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.quantization\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "iu0rBMiBoLSr"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def representative_dataset():\n",
        "    for _ in range(100):\n",
        "      data = np.random.rand(1, 3, 384, 384)\n",
        "      yield [data.astype(np.float32)]"
      ],
      "metadata": {
        "id": "CkTdHbh-rY_b"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "converter.representative_dataset = representative_dataset\n",
        "\n",
        "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
        "converter.inference_input_type = tf.int8   # or tf.uint8\n",
        "converter.inference_output_type = tf.int8  # or tf.uint8\n",
        "\n",
        "tflite_quant_model = converter.convert()"
      ],
      "metadata": {
        "id": "iUP2hFSNn12P"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "open(\"deeplanv3_tensorflow_quant_model.tflite\", \"wb\").write(tflite_quant_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gq4bYO6UuWV4",
        "outputId": "527ef2dc-034a-4c0a-947d-9be8100a024f"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11607432"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    }
  ]
}